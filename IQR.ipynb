{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1aaa0b",
   "metadata": {},
   "source": [
    "üìò What is IQR?\n",
    "The Interquartile Range (IQR) is a measure of statistical dispersion, or how spread out your data is. It tells you the range of the middle 50% of your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f12bedc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8403bd70",
   "metadata": {},
   "source": [
    "IQR=Q3‚àíQ1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4995638",
   "metadata": {},
   "source": [
    "Measures of Spread (Skewness & Kurtosis)\n",
    "These help understand the shape of the data distribution.\n",
    "\n",
    "Skewness: Skewness is a measure of the asymmetry of the probability distribution of a variable. It helps identify whether the data is skewed to the left (negatively skewed), where the tail is longer on the left side, or to the right (positively skewed), where the tail is longer on the right side. A perfectly symmetrical distribution has a skewness of 0. Positive skewness (greater than 0) indicates that the data is concentrated on the left side, while negative skewness (less than 0) indicates concentration on the right side.\n",
    "\n",
    "Skewness (Asymmetry of Data Distribution)\n",
    "\n",
    "\n",
    "\n",
    "df[\"price\"].skew()\n",
    "19.12, a high value, meaning the data is right-skewed (more cheaper hotels, but some very expensive ones).\n",
    "Kurtosis (Peakedness of Data)\n",
    "Kurtosis: Kurtosis is a measure of the \"tailedness\" of a probability distribution, indicating how much data is in the tails compared to a normal distribution. It helps determine the shape of the distribution and whether it has more or fewer extreme values (outliers) than a normal distribution. A positive kurtosis value (greater than 3) indicates heavier tails and more outliers than a normal distribution (leptokurtic), while a negative value (less than 3) suggests lighter tails (platykurtic). A kurtosis of 3 is often considered the baseline for a normal distribution.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"price\"].kurt()\n",
    "\n",
    "585.79, indicating a sharp peak due to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676fc674",
   "metadata": {},
   "source": [
    "Correlation Analysis (Interdependence Between Columns)\n",
    "Correlation helps determine if two numerical variables are related.\n",
    "\n",
    "Correlation: Correlation is a statistical measure that quantifies the degree of association or relationship between two variables. It is typically used for bivariate analysis but can also be relevant in univariate analysis to explore potential relationships that might exist between different subsets or aspects of a single variable. The correlation coefficient, often denoted as \"r,\" ranges from -1 to 1. A positive value indicates a positive correlation (as one variable increases, the other tends to increase), while a negative value indicates a negative correlation (as one variable increases, the other tends to decrease). A value close to zero suggests little to no linear relationship.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.corr()\n",
    "Interpretation of the Correlation Matrix:\n",
    "Price & Minimum Nights: 0.025 (Almost no correlation)\n",
    "Price & Number of Reviews: -0.04 (Slight negative correlation)\n",
    "Number of Reviews & Reviews Per Month: 0.54 (Strong positive correlation)\n",
    "Since most values are close to 0, we conclude that most numerical columns are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7429f9b0",
   "metadata": {},
   "source": [
    "1. üß† What is Gradient Descent?\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost function (error) in linear regression models by iteratively updating the model parameters (weights/coefficients).\n",
    "\n",
    "In simple terms:\n",
    "\"It's how your model learns the best line to fit the data.\"\n",
    "\n",
    "2. üéØ Why Do We Use Gradient Descent?\n",
    "In simple linear regression, we try to find the best slope (m) and intercept (b) for the line:\n",
    "\n",
    "y = mx + b\n",
    "To find the best values of m and b, we minimize a cost function like:\n",
    "\n",
    "Mean Squared Error (MSE) = (1/n) * Œ£(predicted - actual)¬≤\n",
    "For small datasets, you could solve this analytically using the Normal Equation, but with large data, gradient descent scales better.\n",
    "\n",
    "3. üõ†Ô∏è How It Works (Step-by-Step)\n",
    "Initialize model parameters (m, b) with random values.\n",
    "Predict the output using current m, b.\n",
    "Compute the cost using MSE.\n",
    "Calculate gradients of the cost function w.r.t m and b.\n",
    "Update parameters in the opposite direction of the gradient (hence the name \"descent\").\n",
    "Repeat until the cost stops changing much (i.e., convergence).\n",
    "4. ‚úèÔ∏è Formula with Intuition\n",
    "For each iteration:\n",
    "\n",
    "m = m - Œ± * ‚àÇJ/‚àÇm\n",
    "b = b - Œ± * ‚àÇJ/‚àÇb\n",
    "Where:\n",
    "\n",
    "Œ± is the learning rate (how big each step is)\n",
    "‚àÇJ/‚àÇm is the partial derivative of the cost with respect to m\n",
    "‚àÇJ/‚àÇb is the partial derivative with respect to b\n",
    "These derivatives are:\n",
    "\n",
    "‚àÇJ/‚àÇm = -(2/n) * Œ£(xi * (yi - (mxi + b)))\n",
    "‚àÇJ/‚àÇb = -(2/n) * Œ£(yi - (mxi + b))\n",
    "This tells the model how to nudge the line (adjust m and b) to reduce error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99462cc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
